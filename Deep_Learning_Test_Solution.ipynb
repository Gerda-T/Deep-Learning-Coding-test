{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nous alons utiliser SELENIUM pour faire du web scraping sur python"
      ],
      "metadata": {
        "id": "2kMmD-805nJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va commencer par importer les librairies necessaire pour l'exercice"
      ],
      "metadata": {
        "id": "dHBqQSPx53sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium"
      ],
      "metadata": {
        "id": "rzxR8xXAtdxq",
        "outputId": "d53c4baa-4f1d-4f81-c7c5-0016ff64266e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n",
            "\u001b[K     |████████████████████████████████| 958 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure]~=1.26\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 55.1 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 51.2 MB/s \n",
            "\u001b[?25hCollecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 51.5 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (3.10.0.2)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.1 h11-0.13.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.1.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.8 wsproto-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "import time\n",
        "import os\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "sKRPDpDjtd1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enlever les alertes"
      ],
      "metadata": {
        "id": "oPRTEiODKtYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chrome_options = webdriver.ChromeOptions()\n",
        "prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
        "chrome_options.add_experimental_option(\"prefs\",prefs)"
      ],
      "metadata": {
        "id": "OxA4xscmKsc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login sur Facebook"
      ],
      "metadata": {
        "id": "qo8ijUKAK5GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#specifier le path de chromedriver.exe (telecharger et garder le en local)\n",
        "driver = webdriver.Chrome('C:/Users/chromedriver.exe', chrome_options=chrome_options)\n",
        "\n",
        "#ouvrir la page\n",
        "driver.get(\"http://www.facebook.com\")\n",
        "\n",
        "\n",
        "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='email']\")))\n",
        "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='pass']\")))\n",
        "\n",
        "#nom et mdp\n",
        "username.clear()\n",
        "username.send_keys(\"my_username\")\n",
        "password.clear()\n",
        "password.send_keys(\"my_password\")\n",
        "\n",
        "#login\n",
        "button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()\n"
      ],
      "metadata": {
        "id": "0SN5G8bJKskQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing BEAUTIFUL SOUP. C'est une librairie qu'on utilise pour du web scraping"
      ],
      "metadata": {
        "id": "38XixiDrMXpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "5HHzZkWmKsl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_dir=\"C:\\\\\"\n",
        "default_dir = os.path.join(list_dir,\"C:\\Users\\HP\\Desktop\\New folder\\Pictures\")\n",
        "opener = urllib.request.build_opener()\n",
        "urllib.request.install_opener(opener)"
      ],
      "metadata": {
        "id": "p0XampANKspM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = browser.find_element_by_xpath('//*[@id=\"root\"]/div[1]')\n",
        "links = posts.find_elements_by_link_text(\"Actualité intégrale\")\n",
        "pubs = []\n",
        "for link in links:\n",
        "    pub = {}\n",
        "    page_content = requests.get(link.get_attribute('href')).content\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    #get post text\n",
        "    pub['text'] = soup.p.text\n",
        "    pub['image'] = soup.img[\"src\"]\n",
        "    img_url = soup.img[\"src\"]\n",
        "    filename = os.path.join(default_dir, img_url.split(\"/\")[-1])\n",
        "    img_data = opener.open(img_url)\n",
        "    f = open(filename,\"wb\")\n",
        "    f.write(img_data.read())\n",
        "    f.close()\n",
        "    pubs.append(pub)"
      ],
      "metadata": {
        "id": "2VlIIW_FM-50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pubs"
      ],
      "metadata": {
        "id": "57wPZ07n-YEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est recommandé de stocker les textes et les images dans une base MongoDB."
      ],
      "metadata": {
        "id": "WZt0eBgiEmEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "from pymongo import MongoClient"
      ],
      "metadata": {
        "id": "0dU0SpVvAaAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = pymongo.MongoClient(\"mongodb+srv://posts:21011998@ClustP.vqlrt.mongodb.net/\")\n",
        "mydb = client[\"facebook\"]\n",
        "mycol = mydb[\"posts\"]\n",
        "\n",
        "\n",
        "mycol.insert_many(pubs)"
      ],
      "metadata": {
        "id": "5wIkEH6TAaC9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Deep Learning Test Solution",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}